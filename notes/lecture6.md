# 训练神经网络



建立网络，训练，模型评估

> ？为什么需要激活函数。激活函数是用来假如非线性因素的，以避免线性模型的表达能力不足的缺点。

## 激活函数

每个激活函数（或者非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作，以下是几种常见的激活函数：

* sigmoid

公式为$\sigma(x) = 1 / (1 + e^{-x})$，函数图像如下所示，每个元素被压缩到[0-1]

![img](http://cs231n.github.io/assets/nn1/sigmoid.jpeg)

但这种激活函数有两个主要的缺点：一是sigmoid函数饱和会使得梯度消失，通过观测图像我们可以看出来，在自变量越大或者越小的时候，梯度接近0。那么在反向传播要用到局部梯度的时候，相乘的结果也会接近0，这就会kill掉梯度。所以为了防止饱和，需要注意权重矩阵的初始化，避免权重过大；另一个就是sigmoid函数的输出不是零中心的，这个问题会影响梯度下降的效率。因为如果输入数据总是整数（比如在$f=w^Tx+b$中每个元素x>0），那么关于w的梯度在反向传播的时候，将会要么都是正数，要么都是负数，将可能在梯度下降更新权重的时候出现z字型的下降。

第二个问题不是很严重，因为下面介绍的激活函数很多也不是零中心的。

* tanh：

公式为$\tanh(x) = 2 \sigma(2x) -1$，函数图如下：

![img](http://cs231n.github.io/assets/nn1/tanh.jpeg)

该激活函数与sigmoid不同，它能够将值压缩到[-1,1]，也就是其为零中心的，虽然仍然有饱和的问题。tanh比sigmoid更受欢迎



* ReLU

近年来，relu是更加受欢迎的激活函数。它的公式是$f(x)=max(0, x)$，显然这是一个关于0的阈值。

![img](http://cs231n.github.io/assets/nn1/relu.jpeg)

相比于上面的sigmoid和tanh函数，RELU有着巨大的[加速作用](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)，据说是因为它的线性和非饱和的公式所指的；另外，相比于sigmoid和tanh的指数运算，relu只是进行阈值的计算，效率更高。

但也有一个缺点，就是当一个较大的梯度经过relu单元时，更新过参数之后，梯度值为0。此时所有神经元将不能被激活，则神经元死掉了。



* Leaky ReLU

该激活函数的目的就是解决”ReLU“死亡的问题。该函数给出了一个很小的负数梯度，如0.01



* maxout

提取一些其它类型的神经单元，函数形式变为$\max(w_1^Tx+b_1, w_2^Tx + b_2)$，该函数包含了ReLU的优点——线性操作和不饱和，并且不会有死亡单元出现。但是它使得每个神经单元的参数增加了一倍。



> Don't use sigmoid



## 数据预处理

在谈及数据预处理时，先假设三个常用符号，数据矩阵X，尺寸为[NxD]（N是数据样本的数量，D是数据的维度）

### 均值减法

这是预处理常见的做法，从几何的角度来看，就是在每个维度上，将数据云的中心移动到远点，Python中numpy的实现方法：

```python
X-=np.mean(X, axis=0)
```

### 归一化

归一化指的是所有数据维度都归一化。这个预处理操作只有确定不同的输入特征有不同的数据范围才有意义。比如图像处理中，像素的数值范围都是一致的在0~255之间，这里归一化就没有什么必要的。

有两种进行归一化的方法，其一是：先对数据做零中心化处理，然后每个维度都除以其标准差。

```python
X /= np.std(X, axis=0)
```

![img](http://cs231n.github.io/assets/nn2/prepro1.jpeg)

其二就是对每个维度都进行归一化，使得每个维度的最大值和最小值分别为-1和1。

### PCA和白化

pca的作用就是降低维度，步骤是：先对数据进行零中心处理，然后计算协方差矩阵。

```python
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
```

进行scd分解，其中U的列是特征向量，S是装有奇异值的1维向量。

```python
U,S,V = np.linalg.svd(cov)
```

U的列是标准正交向量的集合，因此可以作为标准正交基向量，我们将数据集X投影到基准，就实现了数据降维，并且主成分还在。

```python
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
```

最后一个操作就是白化，通过对特征基准上的数据的每个维度除以其特征值来对数据范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。

```python
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

![img](http://cs231n.github.io/assets/nn2/prepro2.jpeg)

> 注意：进行预处理需要注意的是，先将数据集分成训练/验证/测试集之后，再在训练集上进行预处理操作



## 权重初始化

刚刚是对数据集进行预处理，现在是初始化神经网络的参数。

#### 错误

首先要避免全零初始化，这是因为如果所有参数都是0，那么在网络中的神经元会得到相同的输出，并且它们在反向传播的时候会计算出同样的梯度，进行相同的参数更新。

#### 小随机数初始化

因此权重初始值应该是接近于0，却又不能等于0，通过numpy，我们可以得到这样生成权重向量：

```python
0.01 * np.random.randn(D,H)
```

根据这个式子，每个神经元的权重向量都被初始化为一个随机向量。



## 批量归一化

一般来说，在全连接层或者卷积层后面加入批量归一化。其实可以理解为在网络的每一层之前进行预处理，只不过这里的批量归一化与网络集成在一起了。

之所以要使用批量归一化，是因为即便我们在将数据传送进网络之前进行了预处理操作，随着网络的深入，细小的差别也会被放大，由于每一层的分布都在发生变化，所以每一层都需要适应新的变化。我们令每一层的分布相同有利于网络的训练。另外，让输入分布固定，能减少输入陷入饱和区域的可能性，加入训练。

简单来说，batch normalization就是对相应的activation做规范化操作，使得结果的均值为0，方差为1。而这之后的“scale and shift”就是使得新加入的BN能够有可能还原最初的输入。

![img](https://asdf0982.github.io/2017/06/06/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89/BatchNormalization1.png)

参考资料：https://zhuanlan.zhihu.com/p/24810318





